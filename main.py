from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoConfig
import torch.nn as nn
from transformers import BertPreTrainedModel, BertTokenizer, BertModel
import torch.nn.functional as F
import math
from typing import Dict

app = FastAPI(title="í†µí•© FastAPI Inference ì„œë²„")

# CORS ì„¤ì •: ëª¨ë“  ì¶œì²˜, ë©”ì„œë“œ, í—¤ë” í—ˆìš© (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©í•˜ë ¤ë©´ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

##############################
# 1. í† í°í™” ë° ì²­í‚¹ ì—”ë“œí¬ì¸íŠ¸
##############################

class TokenizeInput(BaseModel):
    text: str
    max_length: int = 300   # ê° ì²­í¬ ë‹¹ ìµœëŒ€ í† í° ìˆ˜
    overlap: int = 50       # ì²­í¬ ê°„ ì¤‘ë³µ í† í° ìˆ˜

# í† í°í™” ì „ìš© í† í¬ë‚˜ì´ì € (ì˜ˆ: "klue/bert-base")
tokenizer_tokenize = AutoTokenizer.from_pretrained("klue/bert-base")

@app.post("/tokenize")
def tokenize_text(data: TokenizeInput):
    text = data.text
    if not text:
        raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    token_ids = tokenizer_tokenize.encode(text, add_special_tokens=False)
    total_tokens = len(token_ids)
    chunks = []
    start = 0
    while start < total_tokens:
        chunk_ids = token_ids[start : start + data.max_length]
        chunk_text = tokenizer_tokenize.decode(chunk_ids, skip_special_tokens=True)
        chunks.append(chunk_text)
        start += (data.max_length - data.overlap)
    return {"chunks": chunks, "total_tokens": total_tokens, "num_chunks": len(chunks)}

################################
# 2. ì¹œë°€ë„ ëª¨ë¸ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
################################

# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (KoBERTIntimacy)
class KoBERTIntimacy(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel(config)
        self.intimacy_regressor = nn.Linear(config.hidden_size, 1)
        self.sigmoid = nn.Sigmoid()
        self.init_weights()
    
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        pooled_output = outputs.pooler_output
        intimacy_score = self.intimacy_regressor(pooled_output).squeeze(1)
        # 0~5 ë²”ìœ„ë¡œ ì •ê·œí™”
        intimacy_score = self.sigmoid(intimacy_score) * 5  
        return {"intimacy_scores": intimacy_score}

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_NAME = "kelly9457/bindly-I-v4"  # ì¹œë°€ë„ ëª¨ë¸ ì´ë¦„ (ì ì ˆíˆ ìˆ˜ì •)
config_model = AutoConfig.from_pretrained(MODEL_NAME, num_labels=1, problem_type="regression")
model = KoBERTIntimacy.from_pretrained(MODEL_NAME, config=config_model)
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
tokenizer_predict = AutoTokenizer.from_pretrained(MODEL_NAME)

class TextInput(BaseModel):
    text: str

@app.post("/predict")
def predict_intimacy(data: TextInput):
    text = data.text
    if not text:
        raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    inputs = tokenizer_predict(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    score = outputs["intimacy_scores"].item()  # 0~5 ë²”ìœ„
    scaled_score = score * 20  # 0~100ìœ¼ë¡œ ë³€í™˜
    return {"raw_score": score, "scaled_score": scaled_score}

class SpeakerModel(BaseModel):
    u1: str
    u2: str

@app.post("/empathy")
def empathy_measure(corpus: str, speaker: SpeakerModel):
    lines = corpus.split('\n')
    u1 = []
    u2 = []
    for idx, text in enumerate(lines):
        startIdx = len(text.split(": ")[0]) + 2
        text = text[startIdx:]

        # u1, u2ì˜ ë¬¸ì¥ì„ ê°ê° ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ìŒ
        if idx % 2 == 0:
            u1.append(text)
        else:
            u2.append(text)

    # [{"speaker": "ê±´ìš°", "score":60}, {"speaker": "ë‚¨í¬", "score":70}]
    dict1 = {"speaker" : speaker.u1, "score": textEmpathyMesureAvg(u1)}
    dict2 = {"speaker": speaker.u1, "score": textEmpathyMesureAvg(u2)}

    return [dict1, dict2]

# parameter : [ë¬¸ì¥1, ë¬¸ì¥2, ë¬¸ì¥3, ...]
# return : 60%    ë“± í‰ê·  ê³µê° ì ìˆ˜
def textEmpathyMesureAvg(lines: list):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        mean_embedding = torch.load("model/empathy_mean_vector.pth", map_location=device)
        print("âœ… ê³µê°í˜• í‰ê·  ë²¡í„° ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print("âŒ í‰ê·  ë²¡í„° ë¡œë“œ ì‹¤íŒ¨:", e)
        return None
    tokenizer = BertTokenizer.from_pretrained("klue/bert-base")
    model = BertModel.from_pretrained("klue/bert-base")
    model.to(device)
    model.eval()
    print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    total_confidence = 0
    for parameter_text in lines:
        predicted_label, confidence = predict_empathy(parameter_text, mean_embedding, tokenizer)
        total_confidence += math.floor(confidence * 100)
    return math.floor(total_confidence / len(lines))


# ì…ë ¥ ë¬¸ì¥ì˜ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
def get_embedding(text, tokenizer):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding='max_length', max_length=512)
    inputs = {key: val.to(device) for key, val in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

# ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜ í•¨ìˆ˜
def predict_empathy(text, mean_embedding, tokenizer):
    print("ğŸ” ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")  # ë””ë²„ê¹…ìš©
    text_embedding = get_embedding(text, tokenizer)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê³µê°í˜• í‰ê·  ë²¡í„°ì™€ ë¹„êµ)
    similarity = F.cosine_similarity(text_embedding, mean_embedding.unsqueeze(0))

    # âœ… ìœ ì‚¬ë„ í‰ê·  ê³„ì‚° (ë¬¸ì¥ ì „ì²´ ìœ ì‚¬ë„)
    similarity_score = similarity.mean().item()

    # ì„ê³„ê°’ ì„¤ì • (ìœ ì‚¬ë„ê°€ 0.5 ì´ìƒì´ë©´ 'ê³µê°í˜•', ì•„ë‹ˆë©´ 'ì§ì„¤í˜•')
    label = "ê³µê°í˜•" if similarity_score >= 0.5 else "ì§ì„¤í˜•"

    print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {label} (ìœ ì‚¬ë„: {similarity_score:.4f})")  # ê²°ê³¼ ì¶œë ¥
    return label, similarity_score

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
